{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0595434e-d46e-42f0-8eb3-b6a2ffc5b220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import skimage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce80c541-ca71-44fc-8996-8d490fabbf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name='dressing-in-order'\n",
    "# os.chdir(f'./{repo_name}')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f2a1219-15fc-478b-bba1-58cbd2e7ffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import importlib\n",
    "import zipfile\n",
    "from models import dior_model  # Assuming dior_model is the name of the module (not the class)\n",
    "import sys\n",
    "import importlib\n",
    "importlib.reload(dior_model)\n",
    "from models.dior_model import DIORModel \n",
    "import os, json\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9ca3477-6b94-46ae-9d42-2e81eb576bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_gdrive(dst_root, fn, gdrive_path, iszip=True):\n",
    "  if not os.path.exists(dst_root):\n",
    "    os.system(\"mkdir {}\".format(dst_root))\n",
    "  if not os.path.exists(\"{}/{}\".format(dst_root, fn)):\n",
    "    os.system(\"gdown {}\".format(gdrive_path))\n",
    "    if iszip:\n",
    "      os.system(\"unzip {}.zip\".format(fn))\n",
    "      os.system(\"rd /s /q {}.zip\".format(fn))\n",
    "    os.system(\"move {} {}/\".format(fn, dst_root))\n",
    "  print(\"download {}.\".format(fn))\n",
    "    \n",
    "\n",
    "if not os.path.exists(\"checkpoints\"):\n",
    "    os.mkdir(\"checkpoints\")\n",
    "    download_from_gdrive(\"checkpoints\", \"DIORv1_64\", \"1MyHq-P0c8zz7ey7p_HTTZKeMie5ZuNlb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a46e738a-1049-497d-a77c-7cd9aa80b496",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phuoc\\miniconda4\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\phuoc\\miniconda4\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load vgg ckpt from torchvision dict.\n",
      "[init] init pre-trained model vgg.\n",
      "initialize network with orthogonal\n",
      "initialize network with orthogonal\n",
      "initialize network with kaiming\n",
      "initialize network with orthogonal\n",
      "[init] frozen net netVGG.\n",
      "[init] frozen net netFlow.\n",
      "[init] frozen net netE_attr.\n",
      "[init] frozen net netE_attr.\n",
      "loading the model from checkpoints\\DIORv1_64\\latest_net_E_attr.pth\n",
      "loading the model from checkpoints\\DIORv1_64\\latest_net_G.pth\n",
      "not exsits checkpoints\\DIORv1_64\\latest_net_VGG.pth\n",
      "loading the model from checkpoints\\DIORv1_64\\latest_net_Flow.pth\n",
      "---------- Networks initialized -------------\n",
      "[Network E_attr] Total number of parameters : 1.191 M\n",
      "[Network G] Total number of parameters : 16.501 M\n",
      "[Network VGG] Total number of parameters : 0.113 M\n",
      "[Network Flow] Total number of parameters : 6.608 M\n",
      "-----------------------------------------------\n",
      "[tensorboard] init tensorboard @ checkpoints\\DIORv1_64\\test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataroot = 'data'\n",
    "exp_name = 'DIORv1_64' \n",
    "epoch = 'latest'\n",
    "netG = 'diorv1' \n",
    "ngf = 64\n",
    "\n",
    "class Opt:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "if True:\n",
    "    opt = Opt()\n",
    "    opt.dataroot = dataroot\n",
    "    opt.isTrain = False\n",
    "    opt.phase = 'test'\n",
    "    opt.n_human_parts = 8; opt.n_kpts = 18; opt.style_nc = 64\n",
    "    opt.n_style_blocks = 4; opt.netG = netG; opt.netE = 'adgan'\n",
    "    opt.ngf = ngf\n",
    "    opt.norm_type = 'instance'; opt.relu_type = 'leakyrelu'\n",
    "    opt.init_type = 'orthogonal'; opt.init_gain = 0.02; opt.gpu_ids = None;\n",
    "    opt.frozen_flownet = True; opt.random_rate = 1; opt.perturb = False; opt.warmup=False\n",
    "    opt.name = exp_name\n",
    "    opt.vgg_path = ''; opt.flownet_path = ''\n",
    "    opt.checkpoints_dir = 'checkpoints'\n",
    "    opt.frozen_enc = True\n",
    "    opt.load_iter = 0\n",
    "    opt.epoch = epoch\n",
    "    opt.verbose = False\n",
    "\n",
    "\n",
    "\n",
    "model = DIORModel(opt)\n",
    "model.setup(opt)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cbd197d-b4c0-4fe0-b527-91447217128c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download testM_lip.\n",
      "download images.\n",
      "download fasion-pairs-test.csv.\n",
      "download fasion-annotation-test.csv.\n",
      "download standard_test_anns.txt.\n"
     ]
    }
   ],
   "source": [
    "download_from_gdrive(\"data\", \"testM_lip\", \"1toeQwAe57LNPTy9EWGG0u1XfTI7qv6b1\")\n",
    "download_from_gdrive(\"data\", \"images\", \"1U2PljA7NE57jcSSzPs21ZurdIPXdYZtN\")\n",
    "download_from_gdrive(\"data\",\"fasion-pairs-test.csv\",\"12fZKGf0kIu5OX3mjC-C3tptxrD8sxm7x\",iszip=False)\n",
    "download_from_gdrive(\"data\",\"fasion-annotation-test.csv\",\"1MxkVFFtNsWFshQp_TA7qwIGEUEUIpYdS\",iszip=False)\n",
    "download_from_gdrive(\"data\",\"standard_test_anns.txt\",\"19nJSHrQuoJZ-6cSl3WEYlhQv6ZsAYG-X\",iszip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12e42173-7afc-48d3-8529-b26b3b79bc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "from datasets.deepfashion_datasets import DFVisualDataset\n",
    "Dataset = DFVisualDataset\n",
    "ds = Dataset(dataroot=dataroot, dim=(256,176), n_human_part=8)\n",
    "\n",
    "\n",
    "inputs = dict()\n",
    "for attr in ds.attr_keys:\n",
    "    inputs[attr] = ds.get_attr_visual_input(attr)\n",
    "    \n",
    "# define some tool functions for I/O\n",
    "def load_img(pid, ds):\n",
    "    if len(pid[0]) < 10: # load pre-selected models\n",
    "        person = inputs[pid[0]]\n",
    "        person = (i for i in person)\n",
    "        pimg, parse, to_pose = person\n",
    "        pimg, parse, to_pose = pimg[pid[1]], parse[pid[1]], to_pose[pid[1]]\n",
    "    else: # load model from scratch\n",
    "        person = ds.get_inputs_by_key(pid[0])\n",
    "        person = (i for i in person)\n",
    "        pimg, parse, to_pose = person\n",
    "    return pimg.squeeze(), parse.squeeze(), to_pose.squeeze()\n",
    "\n",
    "def plot_img(pimg=[], gimgs=[], oimgs=[], gen_img=[], pose=None):\n",
    "    if pose != None:\n",
    "        import utils.pose_utils as pose_utils\n",
    "        print(pose.size())\n",
    "        kpt = pose_utils.draw_pose_from_map(pose.cpu().numpy().transpose(1,2,0),radius=6)\n",
    "        kpt = kpt[0]\n",
    "    if not isinstance(pimg, list):\n",
    "        pimg = [pimg]\n",
    "    if not isinstance(gen_img, list):\n",
    "        gen_img = [gen_img]\n",
    "    out = pimg + gimgs + oimgs + gen_img\n",
    "    if out:\n",
    "        out = torch.cat(out, 2).float().cpu().detach().numpy()\n",
    "        out = (out + 1) / 2 # denormalize\n",
    "        out = np.transpose(out, [1,2,0])\n",
    "\n",
    "        if pose != None:\n",
    "            out = np.concatenate((kpt, out),1)\n",
    "    else:\n",
    "        out = kpt\n",
    "    fig = plt.figure(figsize=(6,4), dpi= 100, facecolor='w', edgecolor='k')\n",
    "    plt.axis('off')\n",
    "    plt.imshow(out)\n",
    "\n",
    "# define dressing-in-order function (the pipeline)\n",
    "def dress_in_order(model, pid, pose_id=None, gids=[], ogids=[], order=[5,1,3,2], perturb=False):\n",
    "    PID = [0,4,6,7]\n",
    "    GID = [2,5,1,3]\n",
    "    # encode person\n",
    "    pimg, parse, from_pose = load_img(pid, ds)\n",
    "    if perturb:\n",
    "        pimg = perturb_images(pimg[None])[0]\n",
    "    if not pose_id:\n",
    "        to_pose = from_pose\n",
    "    else:\n",
    "        to_img, _, to_pose = load_img(pose_id, ds)\n",
    "    psegs = model.encode_attr(pimg[None], parse[None], from_pose[None], to_pose[None], PID)\n",
    "\n",
    "    # encode base garments\n",
    "    gsegs = model.encode_attr(pimg[None], parse[None], from_pose[None], to_pose[None])\n",
    "   \n",
    "    \n",
    "    # swap base garment if any\n",
    "    gimgs = []\n",
    "    for gid in gids:\n",
    "        _,_,k = gid\n",
    "        gimg, gparse, pose =  load_img(gid, ds)\n",
    "        seg = model.encode_single_attr(gimg[None], gparse[None], pose[None], to_pose[None], i=gid[2])\n",
    "        gsegs[gid[2]] = seg\n",
    "        gimgs += [gimg * (gparse == gid[2])]\n",
    "\n",
    "    # encode garment (overlay)\n",
    "    garments = []\n",
    "    over_gsegs = []\n",
    "    oimgs = []\n",
    "    for gid in ogids:\n",
    "        oimg, oparse, pose = load_img(gid, ds)\n",
    "        oimgs += [oimg * (oparse == gid[2])]\n",
    "        seg = model.encode_single_attr(oimg[None], oparse[None], pose[None], to_pose[None], i=gid[2])\n",
    "        over_gsegs += [seg]\n",
    "    \n",
    "    gsegs = [gsegs[i] for i in order] + over_gsegs\n",
    "    gen_img = model.netG(to_pose[None], psegs, gsegs)\n",
    "    \n",
    "    return pimg, gimgs, oimgs, gen_img[0], to_pose\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4352f41-689a-4d6e-bb06-4567c0402c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phuoc\\miniconda4\\Lib\\site-packages\\torch\\nn\\functional.py:4296: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# download from https://www.stickpng.com/img/memes/doge/doge-facing-right\n",
    "import cv2\n",
    "import numpy as np\n",
    "# download_from_gdrive(\"data\",\"doge.png\",\"1ZjEZVWGyLX7Mefrkc03uaEsspP0Nk_EL\",iszip=False)\n",
    "\n",
    "fn = \"doge.png\"\n",
    "pid = ('plain', 3, None)\n",
    "\n",
    "image = cv2.imread(fn, cv2.IMREAD_UNCHANGED) #Read RGBA image\n",
    "# put the print on a blank canvas\n",
    "x,y,h,w = 90,60,80,70\n",
    "image = cv2.resize(image, (w,h))\n",
    "bg = np.zeros((256,176,4))\n",
    "bg[x:x+h,y:y+w] = image\n",
    "image = bg\n",
    "\n",
    "# crop the print image\n",
    "trans_mask = image[:,:,3] != 0\n",
    "image = image[:,:,2::-1].transpose(2,0,1)\n",
    "image = (image / 255.0) * 2 - 1\n",
    "image = image * trans_mask[None]\n",
    "\n",
    "\n",
    "# run DiOr\n",
    "pimg, parse, to_pose =  load_img(pid, ds) \n",
    "psegs = model.encode_attr(pimg[None], parse[None], to_pose[None], to_pose[None], [0,4,6,7])\n",
    "gsegs = model.encode_attr(pimg[None], parse[None], to_pose[None], to_pose[None], [5,1,2])\n",
    "# insert the print\n",
    "print_image = torch.from_numpy(image).float()\n",
    "print_fmap = model.netE_attr(print_image[None], model.netVGG)\n",
    "print_mask = model.netE_attr.segmentor(print_fmap)\n",
    "gsegs = gsegs[:1] + [(print_fmap, torch.sigmoid(print_mask))] + gsegs[1:] \n",
    "# generate\n",
    "gen_img = model.netG(to_pose[None], psegs, gsegs)\n",
    "\n",
    "# construct a copy-and-paste image for comparison\n",
    "paste_img = image + pimg.cpu().detach().numpy() * (1 - trans_mask[None])\n",
    "paste_img = torch.from_numpy(paste_img).float()\n",
    "\n",
    "# display\n",
    "output = torch.cat([pimg, paste_img, gen_img[0]],2)\n",
    "output = output.float().cpu().detach().numpy()\n",
    "output = (output + 1) / 2\n",
    "output = np.transpose((output * 255.0).astype(np.uint8), [1,2,0])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
